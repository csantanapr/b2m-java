{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Build to Manage - Java application monitoring and logging lab During this lab we will instrument a sample java application for logging to use with log analytics tools like Elastic stack as well as for monitoring with Prometheus and Grafana . Instrumentation of the application code for monitoring and logging is part of the general concept we call Build to Manage . It specifies the practice of activities developers can do in order to provide manageability aspects as part of an application release. Lab outline Fork and clone the Github repository with a simple java servlet and review the source code Configure a logging library and add log messages to the application code Enable monitoring features of the WebSphere Liberty Profile Build the application with Apache Maven Configure and run Elastic stack in Docker Compose Configure and run Prometheus and Grafana stack in Docker Compose Build the java application container and integrate with Prometheus and ELK stack Deploy to IBM Cloud Private using helm chart Prerequisites Install the following software on your workstation or use a provided VM. JDK 8 Apache Maven Docker Docker Compose Review the application code and run it locally Logon to GitHub using your user and password. Access the following repository and click Fork . https://github.com/rafal-szypulka/b2m-java From now on you will work on your own fork of the application project https://github.com/ username /b2m-java Clone b2m-java lab repository to your home directory using: cd git clone https://github.com/ username /b2m-java Most of the commands in this lab should be executed from the b2m-java directory: cd b2m-java Review the application code in src/main/java/application/rsapp/checkout.java . This minimalistic application simulates a transaction with random response time on the following URL: http://localhost:9080/rsapp/checkout About 5% of requests should return error and 500 HTTP response code. Run the following command to build the java application and package it into target/rsapp.war : mvn clean install and then mvn liberty:run-server to start the local WLP application server. Use internet browser to access http://localhost:9080/rsapp/checkout The expected output is JSON formatted: { status : RSAP0001I: Transaction OK , transactionTime : 22ms } or: { error : RSAP0010E: Severe problem detected } Refresh the page a couple of times and you should see random transaction response times.","title":"1. Introduction"},{"location":"#build-to-manage-java-application-monitoring-and-logging-lab","text":"During this lab we will instrument a sample java application for logging to use with log analytics tools like Elastic stack as well as for monitoring with Prometheus and Grafana . Instrumentation of the application code for monitoring and logging is part of the general concept we call Build to Manage . It specifies the practice of activities developers can do in order to provide manageability aspects as part of an application release.","title":"Build to Manage - Java application monitoring and logging lab"},{"location":"#lab-outline","text":"Fork and clone the Github repository with a simple java servlet and review the source code Configure a logging library and add log messages to the application code Enable monitoring features of the WebSphere Liberty Profile Build the application with Apache Maven Configure and run Elastic stack in Docker Compose Configure and run Prometheus and Grafana stack in Docker Compose Build the java application container and integrate with Prometheus and ELK stack Deploy to IBM Cloud Private using helm chart","title":"Lab outline"},{"location":"#prerequisites","text":"Install the following software on your workstation or use a provided VM. JDK 8 Apache Maven Docker Docker Compose","title":"Prerequisites"},{"location":"#review-the-application-code-and-run-it-locally","text":"Logon to GitHub using your user and password. Access the following repository and click Fork . https://github.com/rafal-szypulka/b2m-java From now on you will work on your own fork of the application project https://github.com/ username /b2m-java Clone b2m-java lab repository to your home directory using: cd git clone https://github.com/ username /b2m-java Most of the commands in this lab should be executed from the b2m-java directory: cd b2m-java Review the application code in src/main/java/application/rsapp/checkout.java . This minimalistic application simulates a transaction with random response time on the following URL: http://localhost:9080/rsapp/checkout About 5% of requests should return error and 500 HTTP response code. Run the following command to build the java application and package it into target/rsapp.war : mvn clean install and then mvn liberty:run-server to start the local WLP application server. Use internet browser to access http://localhost:9080/rsapp/checkout The expected output is JSON formatted: { status : RSAP0001I: Transaction OK , transactionTime : 22ms } or: { error : RSAP0010E: Severe problem detected } Refresh the page a couple of times and you should see random transaction response times.","title":"Review the application code and run it locally"},{"location":"ICP/","text":"Containerize the app and deploy to IBM Cloud Private Create a Docker container Use provided Dockerfile to build application container: docker build -t b2m-java . Test container locally (make sure you stopped local WLP server). docker run -d -p 9080:9080 b2m-java Access the http://localhost:9080/rsapp/checkout to verify the application is running. Now, our Java application container can be deployed on ICP cluster. Make sure the kubectl client is configured to connect to your ICP cluster. More information here . The b2m-java application container image has been uploaded to public Docker Hub: rszypulka/b2m-java . You can also upload it to the local ICP Container Registry. Deploy b2m-java application to the ICP cluster IBM provides helm charts for WebSphere Liberty and Open Liberty. These helm charts simplify configuration of monitoring and logging and allow to enable built-in monitoring and logging features of Liberty without manual modification of Liberty config files we did earlier. WebSphere Liberty provides also a health feature that allows to implement application health checks and expose a health API that can be used by kubernetes liveness probe as well as external monitoring tools. Setup your CLI environment: cloudctl login -a https:// ICP_CLUSTER_IP :8443 replace with the IP address of you ICP cluster. Verify you can connect your helm client with tiller running in the ICP cluster: $ helm version --tls Client: version.Version{SemVer: v2.9.1 , GitCommit: 20adb27c7c5868466912eebdf6664e7390ebe710 , GitTreeState: clean } Server: version.Version{SemVer: v2.9.1+icp , GitCommit: 8ddf4db6a545dc609539ad8171400f6869c61d8d , GitTreeState: clean } Run the following command to deploy our java application on ICP Cluster: helm install --name btm-java --namespace default ibm-charts/ibm-open-liberty \\ --set monitoring.enabled=true \\ --set image.repository=rszypulka/b2m-java \\ --set image.tag=latest,ssl.enabled=false \\ --set ssl.useClusterSSLConfiguration=false \\ --set ssl.createClusterSSLConfiguration=false \\ --set service.port= 9080 --set service.targetPort= 9080 \\ --set ingress.enabled=false \\ --set jmsService.enabled=false \\ --set iiopService.enabled=false \\ --set logs.persistLogs=false \\ --set logs.persistTransactionLogs=false \\ --set autoscaling.enabled=false \\ --set resources.constraints.enabled=false \\ --set microprofile.health.enabled=true \\ --set sessioncache.hazelcast.enabled=false \\ --set license=accept --tls The command above is just an example and helm chart for WebSphere Liberty provides more options. More information here . You can also deploy the WebSphere Liberty helm chart from the ICP Catalog. Verify status of btm-java helm release: $ helm status btm-java --tls LAST DEPLOYED: Mon Mar 18 15:44:31 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: == v1/ConfigMap NAME DATA AGE btm-java-ibm-open-libert 5 1h == v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE btm-java-ibm-open-libert NodePort 10.0.0.122 none 9080:31936/TCP 1h == v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE btm-java-ibm-open-libert 1 1 1 1 1h == v1/Pod(related) NAME READY STATUS RESTARTS AGE btm-java-ibm-open-libert-b6cdbbd59-qw58b 1/1 Running 0 1h Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace default \\ -o jsonpath= {.spec.ports[0].nodePort} services btm-java-ibm-open-libert) export NODE_IP=$(kubectl get nodes -l proxy=true \\ -o jsonpath= {.items[0].status.addresses[?(@.type==\\ Hostname\\ )].address} ) echo http://$NODE_IP:$NODE_PORT Use an internet browser or curl to access: - Application URL: http://$NODE_IP:$NODE_PORT/rsapp/checkout - Prometheus metrics URL: http://$NODE_IP:$NODE_PORT/metrics - Health API URL: http://$NODE_IP:$NODE_PORT/health Enable monitoring using ICP Prometheus and Grafana Add the following configuration in the monitoring-prometheus ConfigMap, scrape_configs: section. scrape_configs: - job_name: b2m-java scrape_interval: 20s static_configs: - targets: - b2m-java.default.svc:80 labels: service: btm-java group: production Make sure the indentation is correct. Import provided Grafana dashboard ibm-open-liberty-grafana-dashboard.json . Generate ICP application traffic using provided script: ./load_test_icp.sh application_url Use the application_url collected in previous chapter. Access the ICP Grafana console and verify it properly shows metrics. Define kubernetes liveness probe for use with built-in application health check The WebSphere Liberty helm chart configures a default liveness and readiness probes that checks /health route. It can be modified if needed both during helm chart deployment (by configuring image.readinessProbe and image.livenessProbe parameters) or by editing the application deployment. The application container is considered healthy if connection can be established and http response code equals 200 , otherwise it's considered a failure. More information about configuring liveness and readiness probes can be found here . The default liveness probe definition: livenessProbe: failureThreshold: 3 httpGet: path: /health port: 9080 scheme: HTTP initialDelaySeconds: 20 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 Check the URL: http:// node_external_ip : external_nodeport /health to verify current health status. Expected output: {\"checks\":[],\"outcome\":\"UP\"}","title":"5. Deploy to IBM Cloud Private"},{"location":"ICP/#containerize-the-app-and-deploy-to-ibm-cloud-private","text":"","title":"Containerize the app and deploy to IBM Cloud Private"},{"location":"ICP/#create-a-docker-container","text":"Use provided Dockerfile to build application container: docker build -t b2m-java . Test container locally (make sure you stopped local WLP server). docker run -d -p 9080:9080 b2m-java Access the http://localhost:9080/rsapp/checkout to verify the application is running. Now, our Java application container can be deployed on ICP cluster. Make sure the kubectl client is configured to connect to your ICP cluster. More information here . The b2m-java application container image has been uploaded to public Docker Hub: rszypulka/b2m-java . You can also upload it to the local ICP Container Registry.","title":"Create a Docker container"},{"location":"ICP/#deploy-b2m-java-application-to-the-icp-cluster","text":"IBM provides helm charts for WebSphere Liberty and Open Liberty. These helm charts simplify configuration of monitoring and logging and allow to enable built-in monitoring and logging features of Liberty without manual modification of Liberty config files we did earlier. WebSphere Liberty provides also a health feature that allows to implement application health checks and expose a health API that can be used by kubernetes liveness probe as well as external monitoring tools. Setup your CLI environment: cloudctl login -a https:// ICP_CLUSTER_IP :8443 replace with the IP address of you ICP cluster. Verify you can connect your helm client with tiller running in the ICP cluster: $ helm version --tls Client: version.Version{SemVer: v2.9.1 , GitCommit: 20adb27c7c5868466912eebdf6664e7390ebe710 , GitTreeState: clean } Server: version.Version{SemVer: v2.9.1+icp , GitCommit: 8ddf4db6a545dc609539ad8171400f6869c61d8d , GitTreeState: clean } Run the following command to deploy our java application on ICP Cluster: helm install --name btm-java --namespace default ibm-charts/ibm-open-liberty \\ --set monitoring.enabled=true \\ --set image.repository=rszypulka/b2m-java \\ --set image.tag=latest,ssl.enabled=false \\ --set ssl.useClusterSSLConfiguration=false \\ --set ssl.createClusterSSLConfiguration=false \\ --set service.port= 9080 --set service.targetPort= 9080 \\ --set ingress.enabled=false \\ --set jmsService.enabled=false \\ --set iiopService.enabled=false \\ --set logs.persistLogs=false \\ --set logs.persistTransactionLogs=false \\ --set autoscaling.enabled=false \\ --set resources.constraints.enabled=false \\ --set microprofile.health.enabled=true \\ --set sessioncache.hazelcast.enabled=false \\ --set license=accept --tls The command above is just an example and helm chart for WebSphere Liberty provides more options. More information here . You can also deploy the WebSphere Liberty helm chart from the ICP Catalog. Verify status of btm-java helm release: $ helm status btm-java --tls LAST DEPLOYED: Mon Mar 18 15:44:31 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: == v1/ConfigMap NAME DATA AGE btm-java-ibm-open-libert 5 1h == v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE btm-java-ibm-open-libert NodePort 10.0.0.122 none 9080:31936/TCP 1h == v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE btm-java-ibm-open-libert 1 1 1 1 1h == v1/Pod(related) NAME READY STATUS RESTARTS AGE btm-java-ibm-open-libert-b6cdbbd59-qw58b 1/1 Running 0 1h Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace default \\ -o jsonpath= {.spec.ports[0].nodePort} services btm-java-ibm-open-libert) export NODE_IP=$(kubectl get nodes -l proxy=true \\ -o jsonpath= {.items[0].status.addresses[?(@.type==\\ Hostname\\ )].address} ) echo http://$NODE_IP:$NODE_PORT Use an internet browser or curl to access: - Application URL: http://$NODE_IP:$NODE_PORT/rsapp/checkout - Prometheus metrics URL: http://$NODE_IP:$NODE_PORT/metrics - Health API URL: http://$NODE_IP:$NODE_PORT/health","title":"Deploy b2m-java application to the ICP cluster"},{"location":"ICP/#enable-monitoring-using-icp-prometheus-and-grafana","text":"Add the following configuration in the monitoring-prometheus ConfigMap, scrape_configs: section. scrape_configs: - job_name: b2m-java scrape_interval: 20s static_configs: - targets: - b2m-java.default.svc:80 labels: service: btm-java group: production Make sure the indentation is correct. Import provided Grafana dashboard ibm-open-liberty-grafana-dashboard.json . Generate ICP application traffic using provided script: ./load_test_icp.sh application_url Use the application_url collected in previous chapter. Access the ICP Grafana console and verify it properly shows metrics.","title":"Enable monitoring using ICP Prometheus and Grafana"},{"location":"ICP/#define-kubernetes-liveness-probe-for-use-with-built-in-application-health-check","text":"The WebSphere Liberty helm chart configures a default liveness and readiness probes that checks /health route. It can be modified if needed both during helm chart deployment (by configuring image.readinessProbe and image.livenessProbe parameters) or by editing the application deployment. The application container is considered healthy if connection can be established and http response code equals 200 , otherwise it's considered a failure. More information about configuring liveness and readiness probes can be found here . The default liveness probe definition: livenessProbe: failureThreshold: 3 httpGet: path: /health port: 9080 scheme: HTTP initialDelaySeconds: 20 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 Check the URL: http:// node_external_ip : external_nodeport /health to verify current health status. Expected output: {\"checks\":[],\"outcome\":\"UP\"}","title":"Define kubernetes liveness probe for use with built-in application health check"},{"location":"Prometheus-Grafana/","text":"Deploy a local Prometheus and Grafana stack with Docker Compose During this lab we will run the Prometheus and Grafana in Docker Compose. Configuration for this lab is based on https://github.com/vegasbrianc/prometheus . In the lab VM the Prometheus docker compose project was cloned to /root/prometheus . 1). Add scraping job definition to the Prometheus configuration file prometheus/prometheus/prometheus.yml by adding (uncommenting in the lab VM) the following code within scrape_config section: - job_name: btm-java scrape_interval: 20s static_configs: - targets: [ xxx.xxx.xxx.xxx:9080 ] labels: app: b2m-java group: production replace xxx.xxx.xxx.xxx with your own host machine's IP. In the Skytap lab VM, the IP address should be 10.0.0.1 . 2). Start Prometheus Grafana stack: cd ~/prometheus docker-compose down docker-compose up -d Expected output: Creating network prometheus_back-tier with the default driver Creating network prometheus_front-tier with the default driver Creating prometheus_cadvisor_1 ... done Creating prometheus_alertmanager_1 ... done Creating prometheus_node-exporter_1 ... done Creating prometheus_prometheus_1 ... done Creating prometheus_grafana_1 ... done Verify that Prometheus started via: http://localhost:9090 Set the Prometheus datasource in Grafana Logon to Grafana via http://localhost:3000 - user: admin - password: foobar Verify the prometheus datasource configuration in Grafana. If it was not already configured, create a Grafana datasource with this settings: name: Prometheus type: prometheus url: http://localhost:9090 access: browser Configure dashboard Grafana Dashboard to import : ~/b2m-java/ibm-open-liberty-grafana-dashboard.json Generate application traffic using provided script: cd ~/b2m-java ./load_test.sh Expected views: CPU Memory utilization for Liberty Profile: Servlet requests volume and response time:","title":"4. Prometheus and Grafana configuration"},{"location":"Prometheus-Grafana/#deploy-a-local-prometheus-and-grafana-stack-with-docker-compose","text":"During this lab we will run the Prometheus and Grafana in Docker Compose. Configuration for this lab is based on https://github.com/vegasbrianc/prometheus . In the lab VM the Prometheus docker compose project was cloned to /root/prometheus . 1). Add scraping job definition to the Prometheus configuration file prometheus/prometheus/prometheus.yml by adding (uncommenting in the lab VM) the following code within scrape_config section: - job_name: btm-java scrape_interval: 20s static_configs: - targets: [ xxx.xxx.xxx.xxx:9080 ] labels: app: b2m-java group: production replace xxx.xxx.xxx.xxx with your own host machine's IP. In the Skytap lab VM, the IP address should be 10.0.0.1 . 2). Start Prometheus Grafana stack: cd ~/prometheus docker-compose down docker-compose up -d Expected output: Creating network prometheus_back-tier with the default driver Creating network prometheus_front-tier with the default driver Creating prometheus_cadvisor_1 ... done Creating prometheus_alertmanager_1 ... done Creating prometheus_node-exporter_1 ... done Creating prometheus_prometheus_1 ... done Creating prometheus_grafana_1 ... done Verify that Prometheus started via: http://localhost:9090","title":"Deploy a local Prometheus and Grafana stack with Docker Compose"},{"location":"Prometheus-Grafana/#set-the-prometheus-datasource-in-grafana","text":"Logon to Grafana via http://localhost:3000 - user: admin - password: foobar Verify the prometheus datasource configuration in Grafana. If it was not already configured, create a Grafana datasource with this settings: name: Prometheus type: prometheus url: http://localhost:9090 access: browser","title":"Set the Prometheus datasource in Grafana"},{"location":"Prometheus-Grafana/#configure-dashboard","text":"Grafana Dashboard to import : ~/b2m-java/ibm-open-liberty-grafana-dashboard.json Generate application traffic using provided script: cd ~/b2m-java ./load_test.sh Expected views: CPU Memory utilization for Liberty Profile: Servlet requests volume and response time:","title":"Configure dashboard"},{"location":"logging/","text":"A production service should have both logging and monitoring. Monitoring provides a real-time and historical view on the system and application state, and alerts you in case a situation is met. In most cases, a monitoring alert is simply a trigger for you to start an investigation. Monitoring shows the symptoms of problems. Logs provide details and state on individual transactions, so you can fully understand the cause of problems. Logs provide visibility into the behavior of a running app, they are one of the most fundamental tools for debugging and finding issues within your application. If structured correctly, logs can contain a wealth of information about a specific event. Logs can tell us not only when the event took place, but also provide us with details as to the root cause. Therefore, it is important that the log entries are readable to humans and machines. According to the 12-factor application guidelines, logs are the stream of aggregated, time-ordered events. A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage log files. Instead, each running process writes its event stream, unbuffered, to stdout. If you deviate from these guidelines, make sure that you address the operational needs for log files, such as logging to local files and applying log rotation policies. Configure the logging library Java takes a customizable and extensible approach to logging. While Java provides a basic logging API through the java.util.logging package, you can easily use one or more alternative logging solutions instead. In this lab we will use java.util.logging . Example implementation of logging Look for the complete code in final/checkout.complete.java in case of problems. Add the following line at the beginning of src/main/java/application/rsapp/checkout.java (after package statement), to load the logging module: import java.util.logging.Logger ; then declare logger at the top of the class: Logger logger = Logger . getLogger ( rsapp.checkout ); When you want to emit the log entry, call the logger with appropriate log level and message: logger . info ( msg ); or logger.severe(msg); Uncomment the logger calls within src/main/java/application/rsapp/checkout.java . Look for the complete code with logging in final/checkout.complete.java in case of problems. We recommend to format logs in JSON, so it will be easily readable for log analytics software like Elastic stack. For WebSphere Liberty Profile it can be defined globally via environment variables. More information here . Create a server.env file within b2m-java directory with the following contents: WLP_LOGGING_CONSOLE_FORMAT=json WLP_LOGGING_CONSOLE_SOURCE=message,accessLog,ffdc,trace WLP_LOGGING_CONSOLE_LOGLEVEL=info and add instruction to copy this file to the application container to Dockerfile : COPY server.env /config/ Check the final/Dockerfile in case of problems. Review the src/main/liberty/config/server.xml file. These lines are related to logging configuration: logging traceSpecification= *=info / httpAccessLogging id= accessLogging / httpEndpoint httpPort= 9080 httpsPort= 9443 host= * id= defaultHttpEndpoint accessLoggingRef= accessLogging / /httpEndpoint traceSpecification specifies the trace level and httpAccessLogging enables WLP access log (disabled by default). Remember to compare your changes to src/main/java/application/rsapp/checkout.java with final/checkout-complete.java . Verify your Dockerfile with final/Dockerfile . Re-build the target/rsapp.war file with: cd ~/b2m-java mvn clean install Create a Docker image for java application Use modified Dockerfile to build application container: cd ~/b2m-java docker build -t b2m-java . Integrate with the Elastic stack The following procedure shows how to send the application logs to the local Elastic stack running in Docker. Deploy a local Elastic stack with Docker Compose During this lab we will run the Elastic Stack (Elasticsearch, Logstash, Kibana) in the Docker Compose. Configuration for this lab is based on https://github.com/deviantony/docker-elk . In the lab VM the Elastic Stack docker compose project was cloned to /root/docker-elk . Briefly review the simple logstash configuration we use for this lab: /root/docker-elk/logstash/pipeline/logstash.conf : input { gelf { port = 5000 } } filter { json { source = message } #we need level field in a numeric format mutate { gsub = [ level , info , 6, level , error , 3 ] } mutate { convert = { level = integer } } } output { elasticsearch { hosts = elasticsearch:9200 } stdout { codec = rubydebug } } The above will reconfigure logstash to use gelf (Graylog Extended Log Format) protocol supported by Docker log driver, so we can directly stream application logs to Logstash using gelf . 1). Start Elastic stack: cd ~/docker-elk docker-compose up -d Expected output: Creating network docker-elk_elk with driver bridge Creating docker-elk_elasticsearch_1 ... done Creating docker-elk_kibana_1 ... done Creating docker-elk_logstash_1 ... done 2). Verify you can access Kibana on http://localhost:5601 . Start the node.js application container and forward logs to Elastic stack Start the application container with this command: docker run --name btm-java -d -p 9080:9080 --log-driver=gelf \\ --log-opt gelf-address=udp://localhost:5000 b2m-java Simulate a couple fo transactions using Firefox or curl by accessing http://localhost:9080/rsapp/checkout : for i in {1..10}; do curl http://localhost:9080/rsapp/checkout; done and check if you can see application log records in Kibana - Dashboards - Liberty-traffic . In the lab vm environment, the Elastic stack has been preconfigured, so the example Dashboard and Visualizations should be available in Kibana out of the box. and Liberty problems : In case of problems, you can also import Kibana configuration using provided Kibana dashboards: ibm-open-liberty-kibana5-problems-dashboard.json and ibm-open-liberty-kibana5-problems-dashboard.json Go to Kibana: http://localhost:5601 Click on Management - Saved Objects - Import Select btm-nodejs-kibana.json Commit your changes to your GiHub repository: cd ~/b2m-java git config --global user.email your_github_email git commit -am I added logging to my app! git push Access your Github via web browser and verify that you see recent updates and history of changes. Stop and remove the java app Docker container before starting the next exercise. docker stop btm-java docker rm btm-java","title":"2. WLP logging and Elastic stack integration"},{"location":"logging/#configure-the-logging-library","text":"Java takes a customizable and extensible approach to logging. While Java provides a basic logging API through the java.util.logging package, you can easily use one or more alternative logging solutions instead. In this lab we will use java.util.logging .","title":"Configure the logging library"},{"location":"logging/#example-implementation-of-logging","text":"Look for the complete code in final/checkout.complete.java in case of problems. Add the following line at the beginning of src/main/java/application/rsapp/checkout.java (after package statement), to load the logging module: import java.util.logging.Logger ; then declare logger at the top of the class: Logger logger = Logger . getLogger ( rsapp.checkout ); When you want to emit the log entry, call the logger with appropriate log level and message: logger . info ( msg ); or logger.severe(msg); Uncomment the logger calls within src/main/java/application/rsapp/checkout.java . Look for the complete code with logging in final/checkout.complete.java in case of problems. We recommend to format logs in JSON, so it will be easily readable for log analytics software like Elastic stack. For WebSphere Liberty Profile it can be defined globally via environment variables. More information here . Create a server.env file within b2m-java directory with the following contents: WLP_LOGGING_CONSOLE_FORMAT=json WLP_LOGGING_CONSOLE_SOURCE=message,accessLog,ffdc,trace WLP_LOGGING_CONSOLE_LOGLEVEL=info and add instruction to copy this file to the application container to Dockerfile : COPY server.env /config/ Check the final/Dockerfile in case of problems. Review the src/main/liberty/config/server.xml file. These lines are related to logging configuration: logging traceSpecification= *=info / httpAccessLogging id= accessLogging / httpEndpoint httpPort= 9080 httpsPort= 9443 host= * id= defaultHttpEndpoint accessLoggingRef= accessLogging / /httpEndpoint traceSpecification specifies the trace level and httpAccessLogging enables WLP access log (disabled by default). Remember to compare your changes to src/main/java/application/rsapp/checkout.java with final/checkout-complete.java . Verify your Dockerfile with final/Dockerfile . Re-build the target/rsapp.war file with: cd ~/b2m-java mvn clean install","title":"Example implementation of logging"},{"location":"logging/#create-a-docker-image-for-java-application","text":"Use modified Dockerfile to build application container: cd ~/b2m-java docker build -t b2m-java .","title":"Create a Docker image for java application"},{"location":"logging/#integrate-with-the-elastic-stack","text":"The following procedure shows how to send the application logs to the local Elastic stack running in Docker.","title":"Integrate with the Elastic stack"},{"location":"logging/#deploy-a-local-elastic-stack-with-docker-compose","text":"During this lab we will run the Elastic Stack (Elasticsearch, Logstash, Kibana) in the Docker Compose. Configuration for this lab is based on https://github.com/deviantony/docker-elk . In the lab VM the Elastic Stack docker compose project was cloned to /root/docker-elk . Briefly review the simple logstash configuration we use for this lab: /root/docker-elk/logstash/pipeline/logstash.conf : input { gelf { port = 5000 } } filter { json { source = message } #we need level field in a numeric format mutate { gsub = [ level , info , 6, level , error , 3 ] } mutate { convert = { level = integer } } } output { elasticsearch { hosts = elasticsearch:9200 } stdout { codec = rubydebug } } The above will reconfigure logstash to use gelf (Graylog Extended Log Format) protocol supported by Docker log driver, so we can directly stream application logs to Logstash using gelf . 1). Start Elastic stack: cd ~/docker-elk docker-compose up -d Expected output: Creating network docker-elk_elk with driver bridge Creating docker-elk_elasticsearch_1 ... done Creating docker-elk_kibana_1 ... done Creating docker-elk_logstash_1 ... done 2). Verify you can access Kibana on http://localhost:5601 .","title":"Deploy a local Elastic stack with Docker Compose"},{"location":"logging/#start-the-nodejs-application-container-and-forward-logs-to-elastic-stack","text":"Start the application container with this command: docker run --name btm-java -d -p 9080:9080 --log-driver=gelf \\ --log-opt gelf-address=udp://localhost:5000 b2m-java Simulate a couple fo transactions using Firefox or curl by accessing http://localhost:9080/rsapp/checkout : for i in {1..10}; do curl http://localhost:9080/rsapp/checkout; done and check if you can see application log records in Kibana - Dashboards - Liberty-traffic . In the lab vm environment, the Elastic stack has been preconfigured, so the example Dashboard and Visualizations should be available in Kibana out of the box. and Liberty problems : In case of problems, you can also import Kibana configuration using provided Kibana dashboards: ibm-open-liberty-kibana5-problems-dashboard.json and ibm-open-liberty-kibana5-problems-dashboard.json Go to Kibana: http://localhost:5601 Click on Management - Saved Objects - Import Select btm-nodejs-kibana.json Commit your changes to your GiHub repository: cd ~/b2m-java git config --global user.email your_github_email git commit -am I added logging to my app! git push Access your Github via web browser and verify that you see recent updates and history of changes. Stop and remove the java app Docker container before starting the next exercise. docker stop btm-java docker rm btm-java","title":"Start the node.js application container and forward logs to Elastic stack"},{"location":"monitoring-instrumentation/","text":"mpMetrics and monitor features of Liberty Profile There are a number of ways to instrument the java microservice code for monitoring metrics collection. In this lab we will use the WLP provided features: mpMetrics-1.1 and monitor-1.0 These features provides a /metrics REST interface that conforms to the or MicroProfile metrics 1.1 specification. Application developers can add their own custom metrics, by using the MicroProfile metrics API, alongside the metrics provided by Liberty. More information here Enable Prometheus metrics for the WLP application Go to the directory where our b2m-java application has been cloned. cd ~/b2m-java and edit the application server configuration file src/main/liberty/config/server.xml . Uncomment these three lines: feature mpMetrics-1.1 /feature feature monitor-1.0 /feature feature mpHealth-1.0 /feature Test the application locally: cd ~/b2m-java mvn clean install mvn liberty:run-server Access this URL via Firefox or curl : http://localhost:9080/metrics . The output should be similar to: TYPE base:classloader_total_loaded_class_count counter # HELP base:classloader_total_loaded_class_count Displays the total number of classes that have been loaded since the Java virtual machine has started execution. base:classloader_total_loaded_class_count 7890 # TYPE base:gc_global_count counter # HELP base:gc_global_count Displays the total number of collections that have occurred. This attribute lists -1 if the collection count is undefined for this collector. base:gc_global_count 6 # TYPE base:cpu_system_load_average gauge # HELP base:cpu_system_load_average Displays the system load average for the last minute. The system load average is the sum of the number of runnable entities queued to the available processors and the number of runnable entities running on the available processors averaged over a period of time. The way in which the load average is calculated is operating system specific but is typically a damped time-dependent average. If the load average is not available, a negative value is displayed. This attribute is designed to provide a hint about the system load and may be queried frequently. The load average may be unavailable on some platform where it is expensive to implement this method. base:cpu_system_load_average 0.15 # TYPE base:thread_count counter # HELP base:thread_count Displays the current number of live threads including both daemon and non-daemon threads. base:thread_count 48 # TYPE base:classloader_current_loaded_class_count counter # HELP base:classloader_current_loaded_class_count Displays the number of classes that are currently loaded in the Java virtual machine. base:classloader_current_loaded_class_count 7890 # TYPE base:gc_scavenge_time_seconds gauge # HELP base:gc_scavenge_time_seconds Displays the approximate accumulated collection elapsed time in milliseconds. This attribute displays -1 if the collection elapsed time is undefined for this collector. The Java virtual machine implementation may use a high resolution timer to measure the elapsed time. This attribute may display the same value even if the collection count has been incremented if the collection elapsed time is very short. base:gc_scavenge_time_seconds 0.463 (...) Stop the WLP server with ctrl-c and build the docker image using provided Dockerfile : docker stop btm-java docker rm btm-java docker build -t b2m-java . docker run --name btm-java -d -p 9080:9080 --log-driver=gelf \\ --log-opt gelf-address=udp://localhost:5000 b2m-java Commit the changes to your GiHub repository: cd ~/b2m-java git commit -am I added monitoring instrumentation to my app! git push","title":"3. Java application monitoring instrumentation"},{"location":"monitoring-instrumentation/#mpmetrics-and-monitor-features-of-liberty-profile","text":"There are a number of ways to instrument the java microservice code for monitoring metrics collection. In this lab we will use the WLP provided features: mpMetrics-1.1 and monitor-1.0 These features provides a /metrics REST interface that conforms to the or MicroProfile metrics 1.1 specification. Application developers can add their own custom metrics, by using the MicroProfile metrics API, alongside the metrics provided by Liberty. More information here","title":"mpMetrics and monitor features of Liberty Profile"},{"location":"monitoring-instrumentation/#enable-prometheus-metrics-for-the-wlp-application","text":"Go to the directory where our b2m-java application has been cloned. cd ~/b2m-java and edit the application server configuration file src/main/liberty/config/server.xml . Uncomment these three lines: feature mpMetrics-1.1 /feature feature monitor-1.0 /feature feature mpHealth-1.0 /feature Test the application locally: cd ~/b2m-java mvn clean install mvn liberty:run-server Access this URL via Firefox or curl : http://localhost:9080/metrics . The output should be similar to: TYPE base:classloader_total_loaded_class_count counter # HELP base:classloader_total_loaded_class_count Displays the total number of classes that have been loaded since the Java virtual machine has started execution. base:classloader_total_loaded_class_count 7890 # TYPE base:gc_global_count counter # HELP base:gc_global_count Displays the total number of collections that have occurred. This attribute lists -1 if the collection count is undefined for this collector. base:gc_global_count 6 # TYPE base:cpu_system_load_average gauge # HELP base:cpu_system_load_average Displays the system load average for the last minute. The system load average is the sum of the number of runnable entities queued to the available processors and the number of runnable entities running on the available processors averaged over a period of time. The way in which the load average is calculated is operating system specific but is typically a damped time-dependent average. If the load average is not available, a negative value is displayed. This attribute is designed to provide a hint about the system load and may be queried frequently. The load average may be unavailable on some platform where it is expensive to implement this method. base:cpu_system_load_average 0.15 # TYPE base:thread_count counter # HELP base:thread_count Displays the current number of live threads including both daemon and non-daemon threads. base:thread_count 48 # TYPE base:classloader_current_loaded_class_count counter # HELP base:classloader_current_loaded_class_count Displays the number of classes that are currently loaded in the Java virtual machine. base:classloader_current_loaded_class_count 7890 # TYPE base:gc_scavenge_time_seconds gauge # HELP base:gc_scavenge_time_seconds Displays the approximate accumulated collection elapsed time in milliseconds. This attribute displays -1 if the collection elapsed time is undefined for this collector. The Java virtual machine implementation may use a high resolution timer to measure the elapsed time. This attribute may display the same value even if the collection count has been incremented if the collection elapsed time is very short. base:gc_scavenge_time_seconds 0.463 (...) Stop the WLP server with ctrl-c and build the docker image using provided Dockerfile : docker stop btm-java docker rm btm-java docker build -t b2m-java . docker run --name btm-java -d -p 9080:9080 --log-driver=gelf \\ --log-opt gelf-address=udp://localhost:5000 b2m-java Commit the changes to your GiHub repository: cd ~/b2m-java git commit -am I added monitoring instrumentation to my app! git push","title":"Enable Prometheus metrics for the WLP application"}]}